{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac327d91-b133-4755-8183-f34f9be55e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found: /data/REPOSITORY/dwarf_volumes/storm.cosmo25cmb.4096/storm.cosmo25cmb.4096.004096.0000.z0.000.AHF_particles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pynbody.halo : An error occurred while reading substructure file. Falling back to using the halo info.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found: /data/akaxia/storm/storm.cosmo25cmbsi2s50v35.4096/storm.cosmo25cmbsi2s50v35.4096.004096.z0.000.AHF_particles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pynbody.halo : An error occurred while reading substructure file. Falling back to using the halo info.\n",
      "/home/bk639/miniconda3/envs/pynbody/lib/python3.9/site-packages/pynbody/halo/ahf.py:157: UserWarning: Unable to write AHF_fpos file; performance will be reduced. Pass write_fpos=False to halo constructor to suppress this message.\n",
      "  warnings.warn(\"Unable to write AHF_fpos file; performance will be reduced. Pass write_fpos=False to halo constructor to suppress this message.\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pynbody\n",
    "\n",
    "def validate_file_path(file_path):\n",
    "    \"\"\"Check if a file exists at the given path and print a message.\"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"File found: {file_path}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return False\n",
    "\n",
    "# Simulation paths\n",
    "s1path = '/data/REPOSITORY/dwarf_volumes/storm.cosmo25cmb.4096/storm.cosmo25cmb.4096.004096'\n",
    "s2path = '/data/akaxia/storm/storm.cosmo25cmbsi2s50v35.4096/storm.cosmo25cmbsi2s50v35.4096.004096'\n",
    "\n",
    "# Load the simulations\n",
    "s_1 = pynbody.load(s1path)\n",
    "s_2 = pynbody.load(s2path)\n",
    "\n",
    "# AHF file paths for the substructure files\n",
    "ahf_basename_s1 = '/data/REPOSITORY/dwarf_volumes/storm.cosmo25cmb.4096/storm.cosmo25cmb.4096.004096.0000.z0.000.AHF_particles'\n",
    "ahf_basename_s2 = '/data/akaxia/storm/storm.cosmo25cmbsi2s50v35.4096/storm.cosmo25cmbsi2s50v35.4096.004096.z0.000.AHF_particles'\n",
    "\n",
    "# Validate file paths before loading AHF catalogues\n",
    "if validate_file_path(ahf_basename_s1):\n",
    "    h_1 = pynbody.halo.ahf.AHFCatalogue(s_1, filename=ahf_basename_s1)\n",
    "else:\n",
    "    print(\"Skipping loading for simulation 1 due to missing file.\")\n",
    "\n",
    "if validate_file_path(ahf_basename_s2):\n",
    "    h_2 = pynbody.halo.ahf.AHFCatalogue(s_2, filename=ahf_basename_s2)\n",
    "else:\n",
    "    print(\"Skipping loading for simulation 2 due to missing file.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "515593a8-0606-410c-9769-857f0f2123bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_1 = h_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "110b32c4-9411-486e-92ce-9636b87350df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['phi', 'eps', 'den', 'dtsidm', 'mass', 'pos', 'iord', 'vel', 'nsidm']\n",
      "['phi', 'eps', 'den', 'dtsidm', 'mass', 'pos', 'iord', 'vel', 'nsidm']\n"
     ]
    }
   ],
   "source": [
    "print(h_1[1].dm.loadable_keys())\n",
    "print(h_2[1].dm.loadable_keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f061e4be-065b-4e16-a92c-05c0b2e20055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n1 11533219\n",
      "n1 6729422\n",
      "n1 3984595\n",
      "n1 3049840\n",
      "n1 1771966\n",
      "n1 1574603\n",
      "Halo 1 in Simulation 1 matches Halo 1 in Simulation 2 with 100.00% overlap of particle IDs.\n",
      "Halo 2 in Simulation 1 matches Halo 2 in Simulation 2 with 100.00% overlap of particle IDs.\n",
      "Halo 3 in Simulation 1 matches Halo 3 in Simulation 2 with 100.00% overlap of particle IDs.\n",
      "Halo 4 in Simulation 1 matches Halo 4 in Simulation 2 with 100.00% overlap of particle IDs.\n",
      "Halo 5 in Simulation 1 matches Halo 5 in Simulation 2 with 100.00% overlap of particle IDs.\n",
      "Halo 6 in Simulation 1 matches Halo 6 in Simulation 2 with 100.00% overlap of particle IDs.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "min_overlap_percentage = .5\n",
    "halo_match_counter = {}\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "min_overlap_percentage = 50  # Minimum overlap percentage to consider halos matched\n",
    "halo_match_counter = {}\n",
    "\n",
    "# Iterate over halos in the first simulation\n",
    "for i in range(1, 7):  # Adjust range if needed\n",
    "    iord_1 = h_1[i].dm['iord']\n",
    "    num_particles_1 = len(iord_1)  # Total particles in halo i from sim 1\n",
    "    print('n1', num_particles_1)\n",
    "\n",
    "    # Iterate over halos in the second simulation\n",
    "    for j in range(1, 7):  # Adjust range if needed\n",
    "        iord_2 = h_2[j].dm['iord']\n",
    "        common_particles = np.intersect1d(iord_1, iord_2)\n",
    "        total_common = len(common_particles)\n",
    "        \n",
    "        num_particles_2 = len(iord_2)  # Total particles in halo j from sim 2\n",
    "        \n",
    "        # Calculate overlap as a percentage of each halo's particle count\n",
    "        overlap_1 = (total_common / num_particles_1) * 100\n",
    "        overlap_2 = (total_common / num_particles_2) * 100\n",
    "\n",
    "        # Check if overlap meets the minimum threshold against the larger halo\n",
    "        if max(overlap_1, overlap_2) >= min_overlap_percentage:\n",
    "            # Preventing subhalo-host matches by ensuring the match is significant for the larger halo\n",
    "            if max(num_particles_1, num_particles_2) == num_particles_2 and overlap_1 >= min_overlap_percentage:\n",
    "                if (i, j) not in halo_match_counter:\n",
    "                    halo_match_counter[(i, j)] = overlap_1\n",
    "            elif max(num_particles_1, num_particles_2) == num_particles_1 and overlap_2 >= min_overlap_percentage:\n",
    "                if (i, j) not in halo_match_counter:\n",
    "                    halo_match_counter[(i, j)] = overlap_2\n",
    "\n",
    "\n",
    "# Print the best matches that meet the criteria\n",
    "for match, overlap in sorted(halo_match_counter.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f'Halo {match[0]} in Simulation 1 matches Halo {match[1]} in Simulation 2 with {overlap:.2f}% overlap of particle IDs.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711c0ce4-e24e-4a97-ba5a-e2cc85be97f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
